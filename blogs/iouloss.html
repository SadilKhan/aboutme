<!DOCTYPE HTML>
<html>
	<head>
		<title>Md.Sadil Khan</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../assets/css/noscript.css" /></noscript>
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({
			  tex2jax: {
				skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
				inlineMath: [['$','$']]
			  }
			});
		  </script>
		  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 
		

		
	</head>
	<body class="homepage is-preload">
		
		<div id="page-wrapper">

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="../index.html">Home</a></li>
								<li><a href="../about-me.html">About Me</a></li>
								<li>
									<a href="../projects.html">Projects</a>
								</li>
								
								<li><a href="../publications.html">Publications</a></li>
								<li><a href="../blogs.html">Blogs</a></li>
								<li><a href="../contacts.html">Contact Me</a></li>
							</ul>
						</nav>

				</div>

			<!-- Main -->
				<div class="wrapper style1">

					<div class="container">
						<article id="main" class="special">
							<header>
								<h2><a href="#">IoU Losses</a></h2>
                                <p> Loss Functions for Object Detection</p>
                            </header>

                            <div id="toc_container">
								<p class="toc_title">Contents</p>
								<ul class="toc_list">
								  <li><a href="#First_Point_Header">1.  Why not $l_{n}$?</a>
								</li>
								<li><a href="#Second_Point_Header">2. IoU Loss</a></li>
								<li><a href="#Third_Point_Header">3. GIoU Loss</a></li>
								<li><a href="#Fourth_Point_Header">4. DIoU and CIoU Loss</a></li>
                                <li><a href="#Fifth_Point_Header">5. EIoU Loss</a></li>
								</ul>
								</div>

                            <section>
                                <header>
                                    <h3 id="First_Point_Header">1. Why not $l_{n}$?</h3>
                                </header>
                                <p>
                                    Bounding Box Regression is the most important task in Object Detection.
                                    In conventional object detection networks, a $l_{n}$ norm is used during 
                                    training to evaluate the performance of the detector with 
                                    the IoU (Intersection over Union) metric: $IoU=\frac{|B_{G}\cap B_{P}|}{|B_{G} \cup B_{P}|}$ 
                                    where $B_{g}$ and $B_{d}$ are the ground and predicted bounding boxes, respectively.
                                    However there is no correlation between minimizing $l_{n}$ norm and improving the 
                                    loss associated to the IoU metric, $L_{IoU}=1-IoU(B_{g},B_{d})$[1].
                                </p>
                                <figure> 
                                    <img src="../images/whynotl1.png" width="800"/> 
                                    <figcaption class="figure-caption text-center">Figure 1: Three cases where the $l_{2}$-norm distance between the 
                                        representations of two rectangular bounding boxes, each given by the concatenation of the coordinates of 
                                        two opposite corners, has the same value but IoU and GIoU metrics have very different values [1]</figcaption> 
                                </figure>
                                <p>
                                    In Figure 1, the predicted bounding box 
                                    (black rectangle) and ground truth box (green rectangle) are 
                                    each represented by their top-left and bottom-right corners 
                                    (pointed by arrows), and whose the Cartesian coordinates are 
                                    denoted as $(x_{1} , y_{1} , x_{2} , y_{2})$ and $(x_{1}' , 
                                    y_{1}' , x_{2}' , y_{2}')$, respectively. For simplicity, 
                                    let us assume that the distance, e.g. $l_{2}-norm$, 
                                    between one of the corners of two boxes is fixed. Now, if the 
                                    second corner lies on a circle with fixed radius centered on the 
                                    ground truth box, then the $l_{2}$ loss between the ground truth 
                                    box and the predicted bounding box is the same although their IoU 
                                    values can be different depending upon the positions of top-right 
                                    and bottom-left corners. So, using IOU-loss should be the best 
                                    option since we will then minimize the evaluating metric.
                                </p>
                                <section>
                                    <header>
                                        <h3 id="Second_Point_Header">2. IoU Loss [4]</h3>
                                    </header>
                                    <p>
                                        Generally, for two finite sample sets A and B, their IoU is defined as the intersection $(A \cap B)$ divided by the union $(A \cup B)$ of A and B.
                                    </p>
                                    <p style="text-align: center">
                                        $IoU(A,B)=\frac{|A \cap B|}{|A \cup B|}=\frac{|A \cap B|}{|A|+|B|-|A \cup B|}$
                                    </p>
                                    <p>
                                        For bounding box-level object detection, 
                                        the target object is usually represented by a minimum Bbox 
                                        rectangle in the 2D image. Base on this representation, the 
                                        IoU computation between the ground bounding box $B_{g}=(x_{1} , y_{1} , x_{2} , y_{2} )$ and 
                                        the predicted bounding box $B_{d}=(x_{1}^{\prime} , y_{1}^{\prime} , x_{2}^{\prime} , 
                                        y_{2}^{\prime} )$ is defined as:-
                                    </p>
                                    <p style="text-align: center">
                                        $ IoU(A,B)=\frac{\text{Area of overlap between $B_{g}$ and $B_{d}$}}{\text{Area of union of $B_{g}$ and $B_{d}$}}=$
                                    </p>
                                    <p style="text-align: center">
                                        $\frac{(max(x_{1},x_{1}^{\prime})-min(x_{2},x_{2}^{\prime}))\times (max(y_{1},y_{1}^{\prime})-min(y_{2},y_{2}^{\prime}))}{(x_{2}-x_{1}) (y_{2}-y_{1})+(x_{2}^{\prime}-x_{1}^{\prime}) (y_{2}^{\prime}-y_{1}^{\prime})-(max(x_{1},x_{1}^{\prime})-min(x_{2},x_{2}^{\prime})) (max(y_{1},y_{1}^{\prime})-min(y_{2},y_{2}^{\prime}))}$
                                    </p>
                                    <p>Usually, objects are labeled with axis-aligned BBoxes in the 
                                        ROI dataset. By taking this kind of labels as ground truth, 
                                        the predicted BBoxes are also axis-aligned rectangles. 
                                        For this case, the IoU computation is very easy.</p>

                                    <p>
										<strong><i>A. Loss Function:</i></strong>
                                    </p>
									<p>
										The IOU loss function[4] for the ground bounding box $B_{g}=(x_{1} , y_{1} , x_{2} , y_{2} )$ and the predicted bounding box $B_{d}=(x_{1}^{\prime} , y_{1}^{\prime} , x_{2}^{\prime} , y_{2}^{\prime} )$ is defined as
									</p>
									<p style="text-align: center">
									$L_{IoU}=1-IoU(B_{g},B_{d})$
									</p>
									<p>We have to prove that $L_{IoU}$ is a metric</p>
									<p>
										<ol>
											<li>
												Since $0\leq IoU \leq 1, 0\leq L_{IoU} \leq 1$. 
												So $L_{IoU}$ is non-negative. $L_{IoU}=0 \text{ when } 
												IoU(A,B)=1 \implies A and B are the same rectangle$.
											</li>
											<li>
												$IoU(A,B)=IoU(B,A) \implies L_{IoU}(A,B)=L_{IoU}(B,A)$. 
												So $L_{IoU}$ is symmetric.
											</li>
											<li>
												$L_{IoU}$ satisfies triangle inequality.[5]
											</li>
										</ol>
										So $L_{IoU}$ is a metric.
									</p>
									<p><strong><i>B. Differentiability of IoU Loss:</i></strong></p>
									<p>IOU loss is differentiable and can be backpropagated.
										Let $B_{g}=\{x_{1},y_{1},x_{2},y_{2}\}$ be the ground truth and
										 $B_{d}=\{x_{1}^{\prime} , y_{1}^{\prime} , x_{2}^{\prime} , y_{2}^{\prime}\}$
										be the predicted bounding box.</p>
									<p style="text-align: left">
									$X= \text{Area of } B_{g}=(x_{2}-x_{1}) \times (y_{2}-y_{1})$
									<br>$X^{\prime}= \text{Area of } B_{d}=(x_{2}^{\prime}-
									x_{1}^{\prime})\times (y_{2}^{\prime}-y_{1}^{\prime})$
									<br>$I=(max(x_{1},x_{1}^{\prime})-min(x_{2},x_{2}^{\prime}))
									\times (max(y_{1},y_{1}^{\prime})-min(y_{2},y_{2}^{\prime}))$
									<br> $L_{IoU}=1-\frac{I}{X+X^{\prime}-I}=1-\frac{I}{U}, 
									\text{where } U=X+X^{\prime}-I$
									<br> $\frac{\partial L}{\partial x^{\prime}}=\frac{I(\Delta_{x^{\prime}}X-
									\Delta_{x^{\prime}}I)-U\Delta_{x^{\prime}}I}{U^{2}}$
									<br>$\frac{\partial X}{\partial x_{1}^{\prime}}=-
									(y_{2}^{\prime}-y_{1}^{\prime}), \frac{\partial X}{\partial x_{2}^
									{\prime}}=(y_{2}^{\prime}-y_{1}^{\prime}),\frac{\partial X}
									{\partial y_{2}^{\prime}}=(x_{2}^{\prime}-x_{1}^{\prime}),
									\frac{\partial X}{\partial y_{1}^{\prime}}=-(x_{2}^{\prime}-x_{1}^
									{\prime})$
									</p>

									<p style="text-align: center">
									<br> $\frac{\partial I}{\partial x_{1}^{\prime}}=
									\begin{cases}
									(max(y_{1},y_{1}^{\prime})-min(y_{2},y_{2}^{\prime})) & \text{ if } x_{1}^{\prime}>x_{1}\\
									0 & { Otherwise } \end{cases}$
									<br> $\frac{\partial I}{\partial x_{2}^{\prime}}=
									\begin{cases}
									-(max(y_{1},y_{1}^{\prime})-min(y_{2},y_{2}^{\prime})) & \text{ if } x_{2}>x_{2}^{\prime}\\
									0 & { Otherwise } \end{cases} $

									<br> $\frac{\partial I}{\partial y_{1}^{\prime}}=
									\begin{cases}
									(max(x_{1},x_{1}^{\prime})-min(x_{2},x_{2}^{\prime})) & \text{ if } y_{1}^{\prime}>y_{1}\\
									0 & { Otherwise }
									\end{cases}$
									<br> $\frac{\partial I}{\partial y_{2}^{\prime}}=
									\begin{cases}
									-(max(x_{1},x_{1}^{\prime})-min(x_{2},x_{2}^{\prime})) & \text{ if } y_{2}>y_{2}^{\prime}\\
									0 & { Otherwise }
									\end{cases}$
									</p>
									<p>
										So $L_{IoU}$ can be directly used as the objective function to 
										optimize. It is therefore preferable to use IoU as the objective 
										function for 2D object detection tasks. Given the choice between 
										optimizing a metric itself vs.a surrogate loss function, 
										the optimal choice is the metric itself.
									</p>

                                </section>

								<section>
									<header>
										<h3 id="Third_Point_Header">3. GIoU Loss</h3>
									</header>
									<p>
										However, IOU has two major issues as a metric and loss function.
									</p>
									<p>
										<ol>
											<li>
												If two boxes don't overlap, then their IoU is zero, 
												which doesn't give any indication about the proximity 
												of the two boxes.
											</li>
											<li>
												In case of non-overlapping boxes, since their Iou is zero, 
												the gradient is also zero. 
												So $L_{IoU}$ can't be optimized.
											</li>
										</ol>
									</p>
									<p>
										Generalized IoU (GIoU) addresses these weaknesses of IoU.
									</p>
									<p>
										<strong><i>A. Loss Function:</i></strong>
                                    </p>
									<p>
										The Loss function for the Generalized IoU is defined as follows:-
									</p>
									<p>
										Let A and B be two boxes.
										<br> Let C be the smallest enclosing box.
										<br> $IoU=\frac{|A\cap B|}{|A \cup B|}$.
										<br> $GIoU=IoU-\frac{|C\setminus (A\cup B)|}{|C|}$.
										<br> $L_{GIoU}=1-GIoU$.
									</p>
									<p> Some of the properties of GIoU[1]:-
									</p>
									<ul>
										<li>
											Similar to $L_{IoU}, L_{GIoU}$ is also non-negative, 
											symmetric and satisfies triangle inequality. 
											So $L_{GIoU}$ is a metric.
										</li>
										<li>
											GIoU is a lower bound for IoU. 
											$\forall A,B, GIoU(A,B)\leq IoU(A,B)$ and 
											this lower bound becomes tighter 
											when A and B have a stronger shape 
											similarity <i>i.e</i> $\lim_{A \to B} GIoU(A,B)=IoU(A,B)$.
										</li>
										<li>
											$0\leq IoU(A,B)\leq 1 \implies -1\leq GIoU(A,B)\leq 1$.
										</li>
										<li>
											$GIoU=1 \text{ when } |A\cap B|=|A\cup B|$ <i>i.e</i> when A and B completely overlaps.
										</li>
										<li>
											GIoU tends to -1 as the ratio between occupying 
											regions $|A\cup B|$ and the smallest enclosing box 
											C goes to zero, i.e $\lim_{\frac{|A\cup B|}{C} \to 0} 
											GIoU(A,B)=-1 $
										</li>
										<li>
											$L_{GIoU}$ is differentiable.
										</li>
										<li>
											When IoU=0, i.e boxes don't overlap, 
											$L_{GIoU}=2-\frac{|A\cup B|}{|C|}$. 
											By minimizing $L_{GIoU}$, we are maximizing $\frac{|A\cup B|}{|C|}$ 
											($0\leq \frac{|A\cup B|}{|C|} \leq 1$) which means we are 
											maximizing the region of union $|A\cup B|$ and minimizing the 
											enclosing box area $|C|$, which will be possible if the 
											predicted box goes to the ground truth box.
										</li>

									</ul>

								</section>

								<section>
									<header>
										<h3 id="Fourth_Point_Header">4. DIoU and CIoU Loss</h3>
									</header>
									<p>IoU loss works only for overlapping boxes and the 
										problem of gradient-vaninshing in case of non-overlapping boxes 
										had been solved by GIoU Loss but GIoU loss has several limitations.</p>
										<ul>
											<li>
												Generalized IoU tends to increase the size of the predicted 
												bounding box to cover the target ground truth box. From Figure 2,
												we can see that when the predicted bounding box covers the
												ground truth box then $ L_{GIoU}=L_{IoU}$(Since C=$max(A,B)\implies 
												C\setminus(A\cup B)=\Phi$). 
											</li>
											<li>
												$L_{GIoU}$ converges slowly.
											</li>
										</ul>
									
										<figure> 
											<img src="../images/whynotgiou.jpg" width="800"/> 
											<figcaption class="figure-caption text-center">Figure 2: The 
												<span style="color:greenyellow">green</span>,
												<span style="color:rgb(117, 111, 111)">black</span>,
												<span style="color:cyan">blue</span>,
												<span style="color:red">red</span> represents the ground truth 
												box, the anchor box, the predicted box at ith step 
												when GIoU loss is used,the predicted box at ith step 
												when DIoU loss is used respectively. GIoU tends to extend the box
												to cover the [2]
											</figcaption> 
										</figure>
										<p>
											<strong><i>A. Distance IoU (DIoU) Loss Function[2]:</i></strong>
										</p>
										<p>
											Generally IoU-based loss functions can be defined as
										</p>
										<p style="text-align: center">
											$L=1-IoU+R(B,B^{gt})$
										</p>
										<p>
											where $R(B,B^{gt})$ is a penalty term and $B_{gt}$ and $B$ are the
											ground truth box and predicted box.
										</p>
										<p>
											DIoU minimizes the normalized distance between the centre point 
											of the two bounding boxes. The penalty term is
										</p>
										<p style="text-align: center">
											$R_{DIoU}=\frac{\rho^{2}(b,b_{gt})}{c^{2}}$
										</p>
										<p>
											where $b$ and $b_{gt}$ denote the central points of $B$ and $B_{gt}$, 
											and $c$ is the diagonal length of the smallest enclosing box.
											and $\rho$ is the euclidean distance. So,
										</p>
										<p style="text-align: center">
											$L_{DIoU}=1-IoU+\frac{\rho^{2}(b,b_{gt})}{c^{2}}$
										</p>
										<ul>
											<li>
												$L_{DIoU}$ is scale invariant.
											</li>
											<li>$L_{IoU}=L_{GIoU}=L_{DIoU}=0$ when two boxes are same.</li>
										</ul>
									

									

                            	</section>

                            <section>
								<h3>Bibliography</h3>
								<ol>
									<li>
										<p>Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, Silvio Savarese.</p>
										<a href="https://arxiv.org/abs/1902.09630">Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression.</a>
									</li>
									<li>
										<p>Zheng Z., et al</p>
										<a href="https://arxiv.org/abs/1911.08287">Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression</a>
									</li>
									<li>
										<p>Zhang Y., et al</p>
										<a href="https://arxiv.org/abs/2101.08158">Focal and Efficient IOU Loss for Accurate Bounding Box Regression</a>
									</li>
									<li>
										<p>J Yu, et al</p>
										<a href="https://arxiv.org/abs/1608.01471">UnitBox: An Advanced Object Detection Network
                                        </a>
									</li>
									<li>
										<p>S Kosub </p>
										<a href="https://arxiv.org/pdf/1612.02696.pdf">A note on the triangle inequality for the Jaccard distance
                                        </a>
									</li>
								</ol>
							</section>
							
						</article>

					<hr />
						

			<!-- Footer -->
			<div id="footer">
				<div class="container">
					<div class="row">
						

					</div>
					<hr />
					<div class="row">
						<div class="col-12">

							<!-- Contact -->
								<section class="contact">
									<ul class="icons">
										<li><a href="https://github.com/SadilKhan" class="icon brands fa-github"><span class="label">Twitter</span></a></li>
										<li><a href="https://www.facebook.com/profile.php?id=100043829644032" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
										<li><a href="https://www.instagram.com/ryzenx_sk/" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
										<li><a href="https://www.linkedin.com/in/mohammad-sadil-khan-a96568170/" class="icon brands fa-linkedin-in"><span class="label">Linkedin</span></a></li>
									</ul>
								</section>

							<!-- Copyright -->
								<div class="copyright">
									<ul class="menu">
										<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
									</ul>
								</div>

						</div>

					</div>
				</div>
			</div>

	</div>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/jquery.dropotron.min.js"></script>
			<script src="../assets/js/jquery.scrolly.min.js"></script>
			<script src="../assets/js/jquery.scrollex.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>

	</body>
</html>
